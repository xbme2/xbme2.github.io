<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" />

    

    

    <title>阅读 gpt.c | Hexo</title>
    <meta name="author" content="xbme2" />
    <meta name="keywords" content="" />
    <meta name="description" content="尝试理解GPT2结构体从main函数开始阅读，于是去看关于GPT2结构体的定义12345678910111213141516171819202122232425262728typedef struct &amp;#123;    GPT2Config config;    // the weights (parameters) of the model, and their sizes    ParameterTensors params;    size_t param_sizes[NUM_PARAM" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no" />

    
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    
    
    <link rel="icon" href="/images/web.ico">
    

    <style type="text/css">
    @font-face {
        font-family: 'icomoon';
        src: url("/fonts/icomoon.eot?q628ml");
        src: url("/fonts/icomoon.eot?q628ml#iefix") format('embedded-opentype'),
             url("/fonts/icomoon.ttf?q628ml") format('truetype'),
             url("/fonts/icomoon.woff?q628ml") format('woff'),
             url("/fonts/icomoon.svg?q628ml#icomoon") format('svg');
        font-weight: normal;
        font-style: normal;
    }
    </style>
    
<link rel="stylesheet" href="/css/style.css">


    <!--[if lt IE 9]><style type="text/css">.nav-inner {top:0;}.author-meta {position:static;top:0;}.search-form {height:36px;}</style><script type="text/javascript" src="https://unpkg.com/html5shiv@3.7.3/dist/html5shiv.min.js"></script><![endif]-->
<meta name="generator" content="Hexo 7.2.0"></head>
<body>

    <main class="app">
        <header id="header" class="header clearfix">
    <div id="nav" class="nav">
    <div class="nav-mobile">
        <button id="open-panel" class="open-panel nav-mobile-item"><i class="icon-documents"></i></button>
        <h1 class="nav-mobile-title nav-mobile-item">Hexo</h1>
        <button id="open-menus" class="open-panel nav-mobile-item"><i class="icon-library"></i></button>
    </div>

    <nav id="nav-inner" class="nav-inner">
        
            <a class="nav-item" href="/">
                <span class="nav-text">首页</span>
            </a>
        
            <a class="nav-item" href="/categories/OS">
                <span class="nav-text">OS</span>
            </a>
        
            <a class="nav-item" href="/tags">
                <span class="nav-text">标签</span>
            </a>
        
            <a class="nav-item" href="/archives">
                <span class="nav-text">归档</span>
            </a>
        
            <a class="nav-item" href="/atom.xml">
                <span class="nav-text">订阅</span>
            </a>
        
            <a class="nav-item" href="/about">
                <span class="nav-text">关于</span>
            </a>
        
    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="https://xbme2.github.io"></form>

        
        
        
        

        
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95%E7%90%86%E8%A7%A3GPT2%E7%BB%93%E6%9E%84%E4%BD%93"><span class="toc-number">1.</span> <span class="toc-text">尝试理解GPT2结构体</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-GPT2Config%E7%BB%93%E6%9E%84%E4%BD%93"><span class="toc-number">1.0.1.</span> <span class="toc-text">1. GPT2Config结构体</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-ParameterTensors%E7%BB%93%E6%9E%84%E4%BD%93%EF%BC%9A"><span class="toc-number">1.0.2.</span> <span class="toc-text">2. ParameterTensors结构体：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gpt2-build-from-checkpoint-model-%E2%80%9Cgpt2-124M-bin%E2%80%9D"><span class="toc-number">2.</span> <span class="toc-text">gpt2_build_from_checkpoint(&amp;model, “gpt2_124M.bin”);</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#void-gpt2-forward-GPT2-model-int-inputs-int-B-int-T"><span class="toc-number">3.</span> <span class="toc-text">void gpt2_forward(GPT2 model, int inputs, int B, int T)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%A0%E4%B8%AA%E9%83%A8%E5%88%86"><span class="toc-number">3.1.</span> <span class="toc-text">几个部分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0-%E4%B8%BATransformer%E6%A8%A1%E5%9E%8B%E5%BC%A0%E9%87%8F%EF%BC%88%E7%9F%A9%E9%98%B5%EF%BC%89%E5%88%86%E9%85%8D%E7%A9%BA%E9%97%B4%E5%B9%B6%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%88%EF%BC%9F%EF%BC%89"><span class="toc-number">3.1.1.</span> <span class="toc-text">0.为Transformer模型张量（矩阵）分配空间并初始化（？）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-void-encoder-forward-float-out-int-inp-float-wte-float-wpe-int-B-int-T-int-C-%E5%B0%86%E8%BE%93%E5%85%A5%E8%AF%8D%E4%BA%A4%E7%BB%99%E7%AC%AC%E4%B8%80%E4%B8%AADECORDER%E7%BB%84%E4%BB%B6%E4%B9%8B%E5%89%8D%EF%BC%9A%E5%85%88%E6%89%BE%E5%88%B0%E8%AF%A5%E5%8D%95%E8%AF%8D%E7%9A%84embedding%EF%BC%88wte%EF%BC%89%EF%BC%8C%E5%86%8D%E6%8A%8A%E5%AE%83%E5%92%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88wpe%EF%BC%89%E7%9B%B8%E7%BB%93%E5%90%88%E3%80%82"><span class="toc-number">3.1.2.</span> <span class="toc-text">1.void encoder_forward(float* out,int* inp, float* wte, float* wpe,int B, int T, int C):将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2"><span class="toc-number">3.1.3.</span> <span class="toc-text">2.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3"><span class="toc-number">3.1.4.</span> <span class="toc-text">3.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4"><span class="toc-number">3.1.5.</span> <span class="toc-text">4.</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9C%8B%E4%B8%8D%E4%B8%8B%E5%8E%BB%E8%BF%99%E4%BA%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%9C%E8%A5%BF%E4%BA%86%EF%BC%8C%E6%91%86%E7%83%82%E4%BA%86%EF%BC%8Cmatmul-forward%E5%87%BD%E6%95%B0%E5%B0%B1%E6%98%AF%E8%80%97%E6%97%B6%E6%9C%80%E5%A4%A7%E7%9A%84%EF%BC%8Cperf%E8%B0%83%E8%AF%95%E4%B8%8B%E6%9D%A5%E7%A1%AE%E5%AE%9E%E4%B9%9F%E6%98%AF"><span class="toc-number">3.1.5.1.</span> <span class="toc-text">看不下去这些神经网络的东西了，摆烂了，matmul_forward函数就是耗时最大的，perf调试下来确实也是</span></a></li></ol></li></ol></li></ol></li></ol>
        
    </div>
</aside>

</header>

        <div id="content" class="content">
            <div id="wrapper" class="wrapper" style="max-width: 800px">
                <article class="article" itemscope itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            阅读 gpt.c
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="https://xbme2.github.io/2024/05/17/%E9%98%85%E8%AF%BBgpt_c/index.html">
    
    <i class="icon-calendar vm"></i>
    
    <time class="vm" datetime="2024-05-17T02:50:58.546Z" itemprop="datePublished">2024-05-17</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag vm"></i>
    <a class="article-tag-link" href="/tags/NJU-OS/" rel="tag">NJU OS</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <h2 id="尝试理解GPT2结构体"><a href="#尝试理解GPT2结构体" class="headerlink" title="尝试理解GPT2结构体"></a><centre>尝试理解GPT2结构体</h2><span id="more"></span>
<p>从main函数开始阅读，于是去看关于GPT2结构体的定义</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    GPT2Config config;</span><br><span class="line">    <span class="comment">// the weights (parameters) of the model, and their sizes</span></span><br><span class="line">    ParameterTensors params;</span><br><span class="line">    <span class="type">size_t</span> param_sizes[NUM_PARAMETER_TENSORS];</span><br><span class="line">    <span class="type">float</span>* params_memory;</span><br><span class="line">    <span class="type">int</span> num_parameters;</span><br><span class="line">    <span class="comment">// gradients of the weights</span></span><br><span class="line">    ParameterTensors grads;</span><br><span class="line">    <span class="type">float</span>* grads_memory;</span><br><span class="line">    <span class="comment">// buffers for the AdamW optimizer</span></span><br><span class="line">    <span class="type">float</span>* m_memory;</span><br><span class="line">    <span class="type">float</span>* v_memory;</span><br><span class="line">    <span class="comment">// the activations of the model, and their sizes</span></span><br><span class="line">    ActivationTensors acts;</span><br><span class="line">    <span class="type">size_t</span> act_sizes[NUM_ACTIVATION_TENSORS];</span><br><span class="line">    <span class="type">float</span>* acts_memory;</span><br><span class="line">    <span class="type">int</span> num_activations;</span><br><span class="line">    <span class="comment">// gradients of the activations</span></span><br><span class="line">    ActivationTensors grads_acts;</span><br><span class="line">    <span class="type">float</span>* grads_acts_memory;</span><br><span class="line">    <span class="comment">// other run state configuration</span></span><br><span class="line">    <span class="type">int</span> batch_size; <span class="comment">// the batch size (B) of current forward pass</span></span><br><span class="line">    <span class="type">int</span> seq_len; <span class="comment">// the sequence length (T) of current forward pass</span></span><br><span class="line">    <span class="type">int</span>* inputs; <span class="comment">// the input tokens for the current forward pass</span></span><br><span class="line">    <span class="type">int</span>* targets; <span class="comment">// the target tokens for the current forward pass</span></span><br><span class="line">    <span class="type">float</span> mean_loss; <span class="comment">// after a forward pass with targets, will be populated with the mean loss</span></span><br><span class="line">&#125; GPT2;</span><br></pre></td></tr></table></figure>

<h4 id="1-GPT2Config结构体"><a href="#1-GPT2Config结构体" class="headerlink" title="1. GPT2Config结构体"></a>1. GPT2Config结构体</h4><ul>
<li>定义：<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">int</span> max_seq_len; <span class="comment">// max sequence length, e.g. 1024</span></span><br><span class="line">    <span class="type">int</span> vocab_size; <span class="comment">// vocab size, e.g. 50257</span></span><br><span class="line">    <span class="type">int</span> num_layers; <span class="comment">// number of layers, e.g. 12</span></span><br><span class="line">    <span class="type">int</span> num_heads; <span class="comment">// number of heads in attention, e.g. 12</span></span><br><span class="line">    <span class="type">int</span> channels; <span class="comment">// number of channels, e.g. 768</span></span><br><span class="line">&#125; GPT2Config;</span><br></pre></td></tr></table></figure></li>
<li>解释：<ul>
<li>max_seq_len，GPT-2最大可接受的输入序列（tokens）长度</li>
<li>vocab_size：GPT-2对于每一个输入的token，模型会先从embedding矩阵(wte)中查找输入单词的embedding向量，embedding矩阵可以看做是预训练模型的一部分。<br><img src="https://img-blog.csdnimg.cn/f743f4d544c040799cb76063181bea28.png%22embedding%22" alt="Embedding"><br>$\therefore$ vocab_size是embedding矩阵的行数，也是GPT-2模型的词汇总数</li>
<li>num_layers：GPT-2模型decoder组件数目<br><img src="https://img-blog.csdnimg.cn/c3076574f4a141389d0f761136f7ee6a.png" alt="DECODER"></li>
<li>num_heads:多头注意力的头数目，多头就是将原来一个长的Query、Key、Value向量按照不同位置截取并拆分成短的向量，最后将不同头得到的Z拼接</li>
<li>channels：矩阵长度</li>
</ul>
</li>
</ul>
<h4 id="2-ParameterTensors结构体："><a href="#2-ParameterTensors结构体：" class="headerlink" title="2. ParameterTensors结构体："></a>2. ParameterTensors结构体：</h4><ul>
<li>定义<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// the parameters of the model</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_PARAMETER_TENSORS 16</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">float</span>* wte; <span class="comment">// (V, C)</span></span><br><span class="line">    <span class="type">float</span>* wpe; <span class="comment">// (maxT, C)</span></span><br><span class="line">    <span class="type">float</span>* ln1w; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* ln1b; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* qkvw; <span class="comment">// (L, 3*C, C)</span></span><br><span class="line">    <span class="type">float</span>* qkvb; <span class="comment">// (L, 3*C)</span></span><br><span class="line">    <span class="type">float</span>* attprojw; <span class="comment">// (L, C, C)</span></span><br><span class="line">    <span class="type">float</span>* attprojb; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* ln2w; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* ln2b; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* fcw; <span class="comment">// (L, 4*C, C)</span></span><br><span class="line">    <span class="type">float</span>* fcb; <span class="comment">// (L, 4*C)</span></span><br><span class="line">    <span class="type">float</span>* fcprojw; <span class="comment">// (L, C, 4*C)</span></span><br><span class="line">    <span class="type">float</span>* fcprojb; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* lnfw; <span class="comment">// (C)</span></span><br><span class="line">    <span class="type">float</span>* lnfb; <span class="comment">// (C)</span></span><br><span class="line">&#125; ParameterTensors;</span><br></pre></td></tr></table></figure></li>
<li>解释：<ul>
<li>wpe：位置编码矩阵<br><img src="https://img-blog.csdnimg.cn/a09bb1aeb1be4db996d73e17c1c3d5d9.png#pic_center" alt="WPE"></li>
<li>ln1w, ln1b: 第一个Layer Normalization 层的权重和偏置，维度为(L, C)，其中L是层数，C是隐藏层的维度大小。</li>
</ul>
</li>
</ul>
<h2 id="gpt2-build-from-checkpoint-model-“gpt2-124M-bin”"><a href="#gpt2-build-from-checkpoint-model-“gpt2-124M-bin”" class="headerlink" title="gpt2_build_from_checkpoint(&amp;model, “gpt2_124M.bin”);"></a>gpt2_build_from_checkpoint(&amp;model, “gpt2_124M.bin”);</h2><ul>
<li>作用<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">读取gpt2_124M.bin，给model初始化参数</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="void-gpt2-forward-GPT2-model-int-inputs-int-B-int-T"><a href="#void-gpt2-forward-GPT2-model-int-inputs-int-B-int-T" class="headerlink" title="void gpt2_forward(GPT2 model, int inputs, int B, int T)"></a>void gpt2_forward(GPT2 <em>model, int</em> inputs, int B, int T)</h2><h3 id="几个部分"><a href="#几个部分" class="headerlink" title="几个部分"></a>几个部分</h3><h4 id="0-为Transformer模型张量（矩阵）分配空间并初始化（？）"><a href="#0-为Transformer模型张量（矩阵）分配空间并初始化（？）" class="headerlink" title="0.为Transformer模型张量（矩阵）分配空间并初始化（？）"></a>0.为Transformer模型张量（矩阵）分配空间并初始化（？）</h4><h4 id="1-void-encoder-forward-float-out-int-inp-float-wte-float-wpe-int-B-int-T-int-C-将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。"><a href="#1-void-encoder-forward-float-out-int-inp-float-wte-float-wpe-int-B-int-T-int-C-将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。" class="headerlink" title="1.void encoder_forward(float* out,int* inp, float* wte, float* wpe,int B, int T, int C):将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。"></a>1.void encoder_forward(float* out,int* inp, float* wte, float* wpe,int B, int T, int C):将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。</h4><h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> l = <span class="number">0</span>; l &lt; L; l++) &#123;<span class="comment">//&#125;</span></span><br></pre></td></tr></table></figure>
<p>L是DECORDER组件数</p>
<h4 id="3"><a href="#3" class="headerlink" title="3."></a>3.</h4><ul>
<li>得到每一层组件的权重（矩阵）和激活（？）</li>
</ul>
<h4 id="4"><a href="#4" class="headerlink" title="4."></a>4.</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layernorm_forward(l_ln1, l_ln1_mean, l_ln1_rstd, residual, l_ln1w, l_ln1b, B, T, C);</span><br><span class="line">matmul_forward(l_qkv, l_ln1, l_qkvw, l_qkvb, B, T, C, <span class="number">3</span>*C);</span><br><span class="line">attention_forward(l_atty, l_preatt, l_att, l_qkv, B, T, C, NH);</span><br><span class="line">matmul_forward(l_attproj, l_atty, l_attprojw, l_attprojb, B, T, C, C);</span><br><span class="line">residual_forward(l_residual2, residual, l_attproj, B*T*C);</span><br><span class="line">layernorm_forward(l_ln2, l_ln2_mean, l_ln2_rstd, l_residual2, l_ln2w, l_ln2b, B, T, C);</span><br><span class="line">matmul_forward(l_fch, l_ln2, l_fcw, l_fcb, B, T, C, <span class="number">4</span>*C);</span><br><span class="line">gelu_forward(l_fch_gelu, l_fch, B*T*<span class="number">4</span>*C);</span><br><span class="line">matmul_forward(l_fcproj, l_fch_gelu, l_fcprojw, l_fcprojb, B, T, <span class="number">4</span>*C, C);</span><br><span class="line">residual_forward(l_residual3, l_residual2, l_fcproj, B*T*C);</span><br></pre></td></tr></table></figure>

<h5 id="看不下去这些神经网络的东西了，摆烂了，matmul-forward函数就是耗时最大的，perf调试下来确实也是"><a href="#看不下去这些神经网络的东西了，摆烂了，matmul-forward函数就是耗时最大的，perf调试下来确实也是" class="headerlink" title="看不下去这些神经网络的东西了，摆烂了，matmul_forward函数就是耗时最大的，perf调试下来确实也是"></a>看不下去这些神经网络的东西了，摆烂了，matmul_forward函数就是耗时最大的，perf调试下来确实也是</h5><ol>
<li>matmul_forward就是个矩阵相乘，B恒为1</li>
<li>评测机是四核心，所以将其分为四线程</li>
<li>由于thread.h里规定线程池最多16个，所以在线程回收后要及时将n_减少</li>
</ol>
<p>参考：<a target="_blank" rel="noopener" href="https://lolitasian.blog.csdn.net/article/details/125529598">https://lolitasian.blog.csdn.net/article/details/125529598</a></p>

        
    </section>
</article>



<div class="comments">
    <div id="comments"></div>
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>
    new Gitalk({
        clientID: "7fbe80427f54741e289f",
        clientSecret: "f34ed5fd92e54c9000bd37ba951948cb939deff5",
        repo: "sanonz.github.io",
        owner: "sanonz",
        admin: ["sanonz"],
        id: "b57f91d957d9e391924ad4e5cb89ffdf",
        distractionFreeMode: true,
        title: "阅读 gpt.c",
        body: "https://xbme2.github.io/2024/05/17/%E9%98%85%E8%AF%BBgpt_c/",
        labels: ["NJU OS"]
    }).render('comments');
    </script>
</div>



            </div>
        </div>

        
            
            <a id="pagenext" href="/2024/05/14/hello-world/" class="article-next" title="Hello World"><i class="icon-arrow-right"></i></a>
            
            
            <a id="pageprev" href="/2024/05/21/getline/" class="article-prev" title="从getline开始"><i class="icon-arrow-left"></i></a>
            
        

        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>

    
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?e4027971a230b210f4671f485b33846a";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
</footer>

    </main>

    <script type="text/javascript" src="https://unpkg.com/jquery@1.9.1/jquery.min.js"></script>
    <script type="text/javascript">
    $(function() {
        var nodes = {
            nav: $('#nav'),
            aside: $('#aside'),
            asideInner: $('#aside-inner'),
            navInner: $('#nav-inner')
        };

        var doing = false;
        nodes.asideInner.on('webkitAnimationEnd mozAnimationEnd oAnimationEnd oanimationend animationend', function() {
            if (nodes.aside.hasClass('mobile-open')) {
                nodes.aside.removeClass('mobile-open');
            } else {
                nodes.aside.removeClass('mobile-close panel-show');
            }
            doing = false;
        });
        $('#open-panel, #aside-mask').on('click', function() {
            if (doing) {
                return;
            }

            if (nodes.aside.hasClass('panel-show')) {
                nodes.aside.addClass('mobile-close');
            } else {
                nodes.aside.addClass('mobile-open panel-show');
            }
        });
        $('#open-menus').on('click', function() {
            nodes.navInner.slideToggle('normal', slideDone);
        });

        if (window.innerWidth <= 960) {
            setTimeout(function() {
                nodes.navInner.slideUp('normal', slideDone);
            }, 3000);
        }

        function slideDone() {
            if (nodes.navInner.css('display') !== 'none') {
                nodes.navInner.css('display', '');
            }
        }

        $(window).on('resize', function() {
            if ($(this).width() > 960) {
                nodes.navInner.css('display', '');
            }
        });
    });
    </script>
    
        
<script src="/js/scrollspy.min.js"></script>

        <script type="text/javascript">
        $(document.body).scrollspy({target: '#aside-inner'});

        $(window).on('resize', function() {
            var hw = $('#header').width();
            var ww = $('#wrapper').width();
            var space = ($(this).width() - hw - ww) / 2 / 2;

            var pageprev = $('#pageprev');
            var pagenext = $('#pagenext');
            var avg = (pageprev.width() + pagenext.width()) / 2

            if(space > avg) {
                var len = space - avg / 2;
                var styles = {position: 'fixed', top: '50%', marginTop: - (pageprev.width() + pagenext.width()) / 4}
                pageprev.css($.extend({left: hw + len}, styles));
                pagenext.css($.extend({right: len}, styles));
            } else {
                pageprev.removeAttr('style');
                pagenext.removeAttr('style');
            }
        }).trigger('resize');
        </script>
    

</body>
</html>
