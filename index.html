<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" />

    

    

    <title>Hexo</title>
    <meta name="author" content="xbme2" />
    <meta name="keywords" content="" />
    <meta name="description" content="" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no" />

    
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    
    
    <link rel="icon" href="/images/web.ico">
    

    <style type="text/css">
    @font-face {
        font-family: 'icomoon';
        src: url("/fonts/icomoon.eot?q628ml");
        src: url("/fonts/icomoon.eot?q628ml#iefix") format('embedded-opentype'),
             url("/fonts/icomoon.ttf?q628ml") format('truetype'),
             url("/fonts/icomoon.woff?q628ml") format('woff'),
             url("/fonts/icomoon.svg?q628ml#icomoon") format('svg');
        font-weight: normal;
        font-style: normal;
    }
    </style>
    
<link rel="stylesheet" href="/css/style.css">


    <!--[if lt IE 9]><style type="text/css">.nav-inner {top:0;}.author-meta {position:static;top:0;}.search-form {height:36px;}</style><script type="text/javascript" src="https://unpkg.com/html5shiv@3.7.3/dist/html5shiv.min.js"></script><![endif]-->
<meta name="generator" content="Hexo 7.2.0"></head>
<body>

    <main class="app">
        <header id="header" class="header clearfix">
    <div id="nav" class="nav">
    <div class="nav-mobile">
        <button id="open-panel" class="open-panel nav-mobile-item"><i class="icon-documents"></i></button>
        <h1 class="nav-mobile-title nav-mobile-item">Hexo</h1>
        <button id="open-menus" class="open-panel nav-mobile-item"><i class="icon-library"></i></button>
    </div>

    <nav id="nav-inner" class="nav-inner">
        
            <a class="nav-item active" href="/">
                <span class="nav-text">首页</span>
            </a>
        
            <a class="nav-item" href="/categories/front-end">
                <span class="nav-text">前端</span>
            </a>
        
            <a class="nav-item" href="/tags">
                <span class="nav-text">标签</span>
            </a>
        
            <a class="nav-item" href="/archives">
                <span class="nav-text">归档</span>
            </a>
        
            <a class="nav-item" href="/atom.xml">
                <span class="nav-text">订阅</span>
            </a>
        
            <a class="nav-item" href="/about">
                <span class="nav-text">关于</span>
            </a>
        
    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="https://xbme2.github.io"></form>

        
        

        
        <div class="author-meta">
            
            <div class="author-avatar">
                <a href="/">
                    <img src="/images/image.png" title="xbme2">
                </a>
            </div>
            
            <div class="author-name">xbme2</div>
            <div class="author-work">student</div>
            <div class="author-location">
                <i class="icon-location vm"></i>
                <span class="vm">China</span>
            </div>
            
            <div class="author-thread-wrap">
                <div class="author-threads clearfix">
                    
                        <a class="thread-item" href="https://github.com/xbme2" target="_blank" rel="noopener">
                            <!-- Generated by IcoMoon.io -->
<svg viewBox="0 0 1024 1024" width="38" height="38" fill="currentColor">
<path d="M512 32.12c-265.004 0-479.88 220.23-479.88 492.090 0 217.446 137.536 401.684 328.202 466.81 23.994 4.498 32.778-10.712 32.778-23.78 0-11.782-0.428-42.632-0.642-83.764-133.466 29.778-161.744-65.984-161.744-65.984-21.852-56.772-53.344-71.982-53.344-71.982-43.49-30.636 3.214-29.992 3.214-29.992 48.202 3.428 73.482 50.772 73.482 50.772 42.846 75.196 112.258 53.558 139.68 40.918 4.284-31.706 16.71-53.558 30.42-65.77-106.474-12.426-218.516-54.63-218.516-243.152 0-53.772 18.638-97.69 49.274-131.966-4.928-12.426-21.424-62.556 4.714-130.252 0 0 40.276-13.282 131.966 50.344 38.348-10.926 79.266-16.282 120.184-16.496 40.704 0.214 81.836 5.57 120.184 16.496 91.692-63.626 131.752-50.344 131.752-50.344 26.136 67.698 9.64 117.828 4.714 130.252 30.636 34.492 49.274 78.408 49.274 131.966 0 188.952-112.258 230.514-219.16 242.724 17.138 15.21 32.564 45.202 32.564 91.048 0 65.77-0.642 118.898-0.642 134.966 0 13.068 8.57 28.492 32.992 23.566 191.094-64.912 328.418-249.152 328.418-466.382 0-271.86-214.874-492.090-479.88-492.090z"></path>
</svg>

                        </a>
                    
                        <a class="thread-item" href="mailto:xbme2@126.com" target="_blank" rel="noopener">
                            <!-- Generated by IcoMoon.io -->
<svg viewBox="0 0 1024 1024" width="38" height="38" fill="currentColor">
<path d="M512 0a512 512 0 1 0 0 1024A512 512 0 0 0 512 0zM256 256h512c9.152 0 17.984 1.984 26.176 5.632L512 590.784 229.824 261.632A62.848 62.848 0 0 1 256 256z m-64 448V320l0.128-4.032 187.648 218.944-185.6 185.6A61.888 61.888 0 0 1 192 704z m576 64H256c-5.632 0-11.2-0.768-16.512-2.176L421.632 583.68l90.432 105.472L602.496 583.68l182.144 182.144a61.888 61.888 0 0 1-16.512 2.176z m64-64c0 5.632-0.768 11.2-2.176 16.512l-185.6-185.6 187.648-218.944L832 320v384z"></path>
</svg>

                        </a>
                    
                        <a class="thread-item" href="https://twitter.com/xbme2" target="_blank" rel="noopener">
                            <!-- Generated by IcoMoon.io -->
<svg viewBox="0 0 1024 1024" width="38" height="38" fill="currentColor">
<path d="M512.029 31.011c-263.32 0-476.784 213.502-476.784 476.784 0 263.32 213.464 476.743 476.784 476.743s476.743-213.424 476.743-476.743c0-263.282-213.444-476.784-476.743-476.784zM752.193 411.663c0.251 5.151 0.349 10.319 0.349 15.548 0 158.786-120.856 341.85-341.85 341.85-67.844 0-131.021-19.884-184.188-53.961 9.41 1.104 18.955 1.665 28.656 1.665 56.305 0 108.115-19.208 149.221-51.425-52.567-0.987-96.925-35.741-112.22-83.468 7.32 1.433 14.85 2.149 22.595 2.149 10.959 0 21.569-1.433 31.656-4.201-54.987-11.035-96.402-59.634-96.402-117.796 0-0.524 0-1.025 0.020-1.549 16.186 9.003 34.716 14.404 54.427 15.044-32.258-21.587-53.458-58.317-53.458-100.023 0-22.015 5.925-42.673 16.264-60.408 59.266 72.683 147.807 120.527 247.676 125.521-2.053-8.77-3.118-17.968-3.118-27.378 0-66.333 53.787-120.14 120.158-120.14 34.561 0 65.771 14.599 87.69 37.93 27.378-5.363 53.091-15.393 76.305-29.16-9.003 28.074-28.036 51.618-52.858 66.47 24.338-2.903 47.495-9.37 69.024-18.917-16.070 24.144-36.459 45.306-59.945 62.248z"></path>
</svg>

                        </a>
                    
                </div>
            </div>
            
        </div>
        
    </div>
</aside>

</header>

        <div id="content" class="content">
            <div id="wrapper" class="wrapper" style="max-width: 800px">
                
    <article class="article" itemscope itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            <a href="/2024/05/17/%E9%98%85%E8%AF%BBgpt_c/">阅读 gpt.c</a>
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="https://xbme2.github.io/index.html">
    
    <i class="icon-calendar vm"></i>
    
    <time class="vm" datetime="2024-05-17T02:50:58.546Z" itemprop="datePublished">2024-05-17</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag vm"></i>
    <a class="article-tag-link" href="/tags/OS/" rel="tag">OS</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <h2 id="GPT2结构体"><a href="#GPT2结构体" class="headerlink" title="GPT2结构体"></a><centre>GPT2结构体</h2><p>从main函数开始阅读，于是去看关于GPT2结构体的定义</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    GPT2Config config;</span><br><span class="line">    <span class="comment">// the weights (parameters) of the model, and their sizes</span></span><br><span class="line">    ParameterTensors params;</span><br><span class="line">    <span class="type">size_t</span> param_sizes[NUM_PARAMETER_TENSORS];</span><br><span class="line">    <span class="type">float</span>* params_memory;</span><br><span class="line">    <span class="type">int</span> num_parameters;</span><br><span class="line">    <span class="comment">// gradients of the weights</span></span><br><span class="line">    ParameterTensors grads;</span><br><span class="line">    <span class="type">float</span>* grads_memory;</span><br><span class="line">    <span class="comment">// buffers for the AdamW optimizer</span></span><br><span class="line">    <span class="type">float</span>* m_memory;</span><br><span class="line">    <span class="type">float</span>* v_memory;</span><br><span class="line">    <span class="comment">// the activations of the model, and their sizes</span></span><br><span class="line">    ActivationTensors acts;</span><br><span class="line">    <span class="type">size_t</span> act_sizes[NUM_ACTIVATION_TENSORS];</span><br><span class="line">    <span class="type">float</span>* acts_memory;</span><br><span class="line">    <span class="type">int</span> num_activations;</span><br><span class="line">    <span class="comment">// gradients of the activations</span></span><br><span class="line">    ActivationTensors grads_acts;</span><br><span class="line">    <span class="type">float</span>* grads_acts_memory;</span><br><span class="line">    <span class="comment">// other run state configuration</span></span><br><span class="line">    <span class="type">int</span> batch_size; <span class="comment">// the batch size (B) of current forward pass</span></span><br><span class="line">    <span class="type">int</span> seq_len; <span class="comment">// the sequence length (T) of current forward pass</span></span><br><span class="line">    <span class="type">int</span>* inputs; <span class="comment">// the input tokens for the current forward pass</span></span><br><span class="line">    <span class="type">int</span>* targets; <span class="comment">// the target tokens for the current forward pass</span></span><br><span class="line">    <span class="type">float</span> mean_loss; <span class="comment">// after a forward pass with targets, will be populated with the mean loss</span></span><br><span class="line">&#125; GPT2;</span><br></pre></td></tr></table></figure>

<h4 id="1-GPT2Config结构体"><a href="#1-GPT2Config结构体" class="headerlink" title="1. GPT2Config结构体"></a>1. GPT2Config结构体</h4><ul>
<li>定义：<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">int</span> max_seq_len; <span class="comment">// max sequence length, e.g. 1024</span></span><br><span class="line">    <span class="type">int</span> vocab_size; <span class="comment">// vocab size, e.g. 50257</span></span><br><span class="line">    <span class="type">int</span> num_layers; <span class="comment">// number of layers, e.g. 12</span></span><br><span class="line">    <span class="type">int</span> num_heads; <span class="comment">// number of heads in attention, e.g. 12</span></span><br><span class="line">    <span class="type">int</span> channels; <span class="comment">// number of channels, e.g. 768</span></span><br><span class="line">&#125; GPT2Config;</span><br></pre></td></tr></table></figure></li>
<li>解释：<ul>
<li>max_seq_len，GPT-2最大可接受的输入序列（tokens）长度</li>
<li>vocab_size：GPT-2对于每一个输入的token，模型会先从embedding矩阵(wte)中查找输入单词的embedding向量，embedding矩阵可以看做是预训练模型的一部分。<br><img src="https://img-blog.csdnimg.cn/f743f4d544c040799cb76063181bea28.png%22embedding%22" alt="Embedding"><br>$\therefore$ vocab_size是embedding矩阵的行数，也是GPT-2模型的词汇总数</li>
<li>num_layers：GPT-2模型decoder组件数目<br><img src="https://img-blog.csdnimg.cn/c3076574f4a141389d0f761136f7ee6a.png" alt="DECODER"></li>
<li>num_heads:多头注意力的头数目，多头就是将原来一个长的Query、Key、Value向量按照不同位置截取并拆分成短的向量，最后将不同头得到的Z拼接</li>
<li>channels：矩阵长度</li>
</ul>
</li>
</ul>
<h4 id="2-ParameterTensors结构体："><a href="#2-ParameterTensors结构体：" class="headerlink" title="2. ParameterTensors结构体："></a>2. ParameterTensors结构体：</h4><ul>
<li>定义<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// the parameters of the model</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_PARAMETER_TENSORS 16</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">float</span>* wte; <span class="comment">// (V, C)</span></span><br><span class="line">    <span class="type">float</span>* wpe; <span class="comment">// (maxT, C)</span></span><br><span class="line">    <span class="type">float</span>* ln1w; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* ln1b; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* qkvw; <span class="comment">// (L, 3*C, C)</span></span><br><span class="line">    <span class="type">float</span>* qkvb; <span class="comment">// (L, 3*C)</span></span><br><span class="line">    <span class="type">float</span>* attprojw; <span class="comment">// (L, C, C)</span></span><br><span class="line">    <span class="type">float</span>* attprojb; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* ln2w; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* ln2b; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* fcw; <span class="comment">// (L, 4*C, C)</span></span><br><span class="line">    <span class="type">float</span>* fcb; <span class="comment">// (L, 4*C)</span></span><br><span class="line">    <span class="type">float</span>* fcprojw; <span class="comment">// (L, C, 4*C)</span></span><br><span class="line">    <span class="type">float</span>* fcprojb; <span class="comment">// (L, C)</span></span><br><span class="line">    <span class="type">float</span>* lnfw; <span class="comment">// (C)</span></span><br><span class="line">    <span class="type">float</span>* lnfb; <span class="comment">// (C)</span></span><br><span class="line">&#125; ParameterTensors;</span><br></pre></td></tr></table></figure></li>
<li>解释：<ul>
<li>wpe：位置编码矩阵<br><img src="https://img-blog.csdnimg.cn/a09bb1aeb1be4db996d73e17c1c3d5d9.png#pic_center" alt="WPE"></li>
<li>ln1w, ln1b: 第一个Layer Normalization 层的权重和偏置，维度为(L, C)，其中L是层数，C是隐藏层的维度大小。</li>
</ul>
</li>
</ul>
<h2 id="gpt2-build-from-checkpoint-model-“gpt2-124M-bin”"><a href="#gpt2-build-from-checkpoint-model-“gpt2-124M-bin”" class="headerlink" title="gpt2_build_from_checkpoint(&amp;model, “gpt2_124M.bin”);"></a>gpt2_build_from_checkpoint(&amp;model, “gpt2_124M.bin”);</h2><ul>
<li>作用<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">读取gpt2_124M.bin，给model初始化参数</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="void-gpt2-forward-GPT2-model-int-inputs-int-B-int-T"><a href="#void-gpt2-forward-GPT2-model-int-inputs-int-B-int-T" class="headerlink" title="void gpt2_forward(GPT2 model, int inputs, int B, int T)"></a>void gpt2_forward(GPT2 <em>model, int</em> inputs, int B, int T)</h2><h3 id="几个部分"><a href="#几个部分" class="headerlink" title="几个部分"></a>几个部分</h3><h4 id="0-为Transformer模型张量（矩阵）分配空间并初始化（？）"><a href="#0-为Transformer模型张量（矩阵）分配空间并初始化（？）" class="headerlink" title="0.为Transformer模型张量（矩阵）分配空间并初始化（？）"></a>0.为Transformer模型张量（矩阵）分配空间并初始化（？）</h4><h4 id="1-void-encoder-forward-float-out-int-inp-float-wte-float-wpe-int-B-int-T-int-C-将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。"><a href="#1-void-encoder-forward-float-out-int-inp-float-wte-float-wpe-int-B-int-T-int-C-将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。" class="headerlink" title="1.void encoder_forward(float* out,int* inp, float* wte, float* wpe,int B, int T, int C):将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。"></a>1.void encoder_forward(float* out,int* inp, float* wte, float* wpe,int B, int T, int C):将输入词交给第一个DECORDER组件之前：先找到该单词的embedding（wte），再把它和对应的位置编码（wpe）相结合。</h4><h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> l = <span class="number">0</span>; l &lt; L; l++) &#123;<span class="comment">//&#125;</span></span><br></pre></td></tr></table></figure>
<p>L是DECORDER组件数</p>
<h4 id="3"><a href="#3" class="headerlink" title="3."></a>3.</h4><ul>
<li>得到每一层组件的权重（矩阵）和激活（？）</li>
</ul>
<h4 id="4"><a href="#4" class="headerlink" title="4."></a>4.</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layernorm_forward(l_ln1, l_ln1_mean, l_ln1_rstd, residual, l_ln1w, l_ln1b, B, T, C);</span><br><span class="line">matmul_forward(l_qkv, l_ln1, l_qkvw, l_qkvb, B, T, C, <span class="number">3</span>*C);</span><br><span class="line">attention_forward(l_atty, l_preatt, l_att, l_qkv, B, T, C, NH);</span><br><span class="line">matmul_forward(l_attproj, l_atty, l_attprojw, l_attprojb, B, T, C, C);</span><br><span class="line">residual_forward(l_residual2, residual, l_attproj, B*T*C);</span><br><span class="line">layernorm_forward(l_ln2, l_ln2_mean, l_ln2_rstd, l_residual2, l_ln2w, l_ln2b, B, T, C);</span><br><span class="line">matmul_forward(l_fch, l_ln2, l_fcw, l_fcb, B, T, C, <span class="number">4</span>*C);</span><br><span class="line">gelu_forward(l_fch_gelu, l_fch, B*T*<span class="number">4</span>*C);</span><br><span class="line">matmul_forward(l_fcproj, l_fch_gelu, l_fcprojw, l_fcprojb, B, T, <span class="number">4</span>*C, C);</span><br><span class="line">residual_forward(l_residual3, l_residual2, l_fcproj, B*T*C);</span><br></pre></td></tr></table></figure>

<h5 id="看不下去这些神经网络的东西了，摆烂了，matmul-forward函数就是耗时最大的，perf调试下来确实也是"><a href="#看不下去这些神经网络的东西了，摆烂了，matmul-forward函数就是耗时最大的，perf调试下来确实也是" class="headerlink" title="看不下去这些神经网络的东西了，摆烂了，matmul_forward函数就是耗时最大的，perf调试下来确实也是"></a>看不下去这些神经网络的东西了，摆烂了，matmul_forward函数就是耗时最大的，perf调试下来确实也是</h5><ol>
<li>matmul_forward就是个矩阵相乘，B恒为1</li>
<li>评测机是四核心，所以将其分为四线程</li>
<li>由于thread.h里规定线程池最多16个，所以在线程回收后要及时将n_减少</li>
</ol>

        
    </section>
</article>


    <article class="article" itemscope itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            <a href="/2024/05/14/hello-world/">Hello World</a>
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="https://xbme2.github.io/index.html">
    
    <i class="icon-calendar vm"></i>
    
    <time class="vm" datetime="2024-05-14T03:10:53.571Z" itemprop="datePublished">2024-05-14</time>
</a>

            

        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="It-is-xbme2-blog"><a href="#It-is-xbme2-blog" class="headerlink" title="It is xbme2 blog"></a>It is xbme2 blog</h2><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

        
    </section>
</article>






            </div>
        </div>

        

        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>

    
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?e4027971a230b210f4671f485b33846a";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
</footer>

    </main>

    <script type="text/javascript" src="https://unpkg.com/jquery@1.9.1/jquery.min.js"></script>
    <script type="text/javascript">
    $(function() {
        var nodes = {
            nav: $('#nav'),
            aside: $('#aside'),
            asideInner: $('#aside-inner'),
            navInner: $('#nav-inner')
        };

        var doing = false;
        nodes.asideInner.on('webkitAnimationEnd mozAnimationEnd oAnimationEnd oanimationend animationend', function() {
            if (nodes.aside.hasClass('mobile-open')) {
                nodes.aside.removeClass('mobile-open');
            } else {
                nodes.aside.removeClass('mobile-close panel-show');
            }
            doing = false;
        });
        $('#open-panel, #aside-mask').on('click', function() {
            if (doing) {
                return;
            }

            if (nodes.aside.hasClass('panel-show')) {
                nodes.aside.addClass('mobile-close');
            } else {
                nodes.aside.addClass('mobile-open panel-show');
            }
        });
        $('#open-menus').on('click', function() {
            nodes.navInner.slideToggle('normal', slideDone);
        });

        if (window.innerWidth <= 960) {
            setTimeout(function() {
                nodes.navInner.slideUp('normal', slideDone);
            }, 3000);
        }

        function slideDone() {
            if (nodes.navInner.css('display') !== 'none') {
                nodes.navInner.css('display', '');
            }
        }

        $(window).on('resize', function() {
            if ($(this).width() > 960) {
                nodes.navInner.css('display', '');
            }
        });
    });
    </script>
    

</body>
</html>
